{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\Anaconda3\\envs\\mllecture\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import treebank\n",
    "from nltk.corpus.reader.conll import *\n",
    "import re\n",
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = \"unamb_sent_14_6.conllu\"\n",
    "train_reader = ConllCorpusReader(root = '', fileids = [train_data], columntypes=['ignore', 'ignore', 'words', 'pos', 'chunk'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28881.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents = list(train_reader.iob_sents())\n",
    "len(sents)*(3/4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28881\n",
      "9627\n"
     ]
    }
   ],
   "source": [
    "train_sents = sents[:28881]\n",
    "test_sents = sents[28881: ]\n",
    "print(len(train_sents))\n",
    "print(len(test_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = [[word[0].lower()  for word in sent ] for sent in train_sents ]\n",
    "X_test = [[word[0].lower()  for word in sent ] for sent in test_sents ]\n",
    "Y_train = [[word[1]  for word in sent ] for sent in train_sents ]\n",
    "Y_test = [[word[1]  for word in sent ] for sent in test_sents ]\n",
    "\n",
    "#all words\n",
    "words = list(set([word[0].lower() for sent in train_sents+test_sents for word in sent ]))\n",
    "\n",
    "word2ind = {word: index for index, word in enumerate(words)}\n",
    "ind2word = {index: word for index, word in enumerate(words)}\n",
    "\n",
    "#all labels\n",
    "labels = list(set([word[1] for sent in train_sents for word in sent ]))\n",
    "\n",
    "label2ind = {label: (index + 1) for index, label in enumerate(labels)}\n",
    "ind2label = {(index + 1): label for index, label in enumerate(labels)}\n",
    "\n",
    "maxlen = max([len(sent) for sent in train_sents+test_sents])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creating full sentences to fit them into tokenizer\n",
    "train_sentences=[]\n",
    "for i in X_train:\n",
    "    s = ' '.join(w for w in i)\n",
    "    train_sentences.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#add padding\n",
    "train_sentences.append('pad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.preprocessing.text.Tokenizer at 0x223cfebed68>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(train_sentences)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['« школа злословие » учить прикусить язык', 'сохраниться ли градус дискуссия в новый сезон ?', 'великолепный « школа злословие » вернуться в эфир после летний каникулы в новый формат .', 'в история программа это уже не первый « ребрендинг » .', 'писательница татьяна толстая и сценаристка дуня смирнова вроде бы не вполне соответствовать принятый на российский телевидение стандарт телеведущая .', 'в остальной « школа злословие » представлять себя интервью ведущий с герой выпуск .', 'иногда и в самый дело не без злословие , а по больший часть – разговор « с придыхание » , например в программа с участие борис берман и ильдар жандарев , с который чем далёкий , тем большой « родство » наблюдаться у ведущий « школа … » .', 'потом проект переехать с « культура » на нтв .', 'это помимо явный перемена в вид тут же появившийся рекламный блок , отсутствовавший на « культура » , позволить , с один сторона , расширить круг гость , с другой – изменить тон разговор .', 'набор герой программа расшириться самый причудливый образ , да и « злословие » стать маленький .']\n"
     ]
    }
   ],
   "source": [
    "print(train_sentences[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5, 294, 4897, 6, 2284, 251], [3009, 4028, 2388, 1, 44, 684], [5, 294, 4897, 6, 496, 1, 1831, 62, 1328, 1, 44, 1193], [1, 120, 135, 38, 54, 7, 47, 5, 6], [4898, 1771, 2, 64, 7, 725, 1030, 955, 4, 52, 956, 1832], [1, 461, 5, 294, 4897, 6, 392, 65, 808, 4402, 8, 603, 1417], [497, 2, 1, 67, 76, 7, 85, 4897, 14, 11, 2001, 119, 94, 726, 5, 8, 6, 167, 1, 135, 8, 270, 1243, 2, 8, 13, 252, 654, 142, 5, 6, 1244, 32, 4402, 5, 294, 59, 6], [266, 123, 4029, 8, 5, 313, 6, 4], [38, 831, 3468, 2389, 1, 132, 203, 68, 4030, 1691, 2081, 4, 5, 313, 6, 1004, 8, 35, 114, 685, 1031, 8, 43, 94, 3710, 3227, 726], [2390, 603, 135, 67, 159, 99, 2, 5, 4897, 6, 46, 300]]\n"
     ]
    }
   ],
   "source": [
    "sequences = tokenizer.texts_to_sequences(train_sentences)\n",
    "print(sequences[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#has its own number now\n",
    "pad_index= word_index.get('pad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#remaking labels\n",
    "\n",
    "def encode(x, n):\n",
    "    result = np.zeros(n)\n",
    "    result[x] = 1\n",
    "    return result\n",
    "\n",
    "Y_train_all=[]\n",
    "for i in Y_train:\n",
    "    for j in i:\n",
    "        Y_train_all.append(j)\n",
    "Y_train_toind=[label2ind[s] for s in Y_train_all]\n",
    "Y_train_encode=[encode(int(i), 15) for i in Y_train_toind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350355\n"
     ]
    }
   ],
   "source": [
    "print(len(Y_train_encode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_test_all=[]\n",
    "for i in Y_test:\n",
    "    for j in i:\n",
    "        Y_test_all.append(j)\n",
    "Y_test_toind=[label2ind[s] for s in Y_test_all]\n",
    "Y_test_encode=[encode(int(i), 15) for i in Y_test_toind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107228\n"
     ]
    }
   ],
   "source": [
    "print(len(Y_test_encode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_dim=300\n",
    "emb_path = 'wiki.ru.vec'\n",
    "\n",
    "words = []\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(emb_path, encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    if len(values) == 301:\n",
    "        word = values[0]\n",
    "        words.append(word)\n",
    "        coefs = np.asarray(values[1:(embedding_dim+1)], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_window(word_list, k):\n",
    "    windows = []\n",
    "    hk = int((k-1)/2)\n",
    "    for i in range(len(word_list)):\n",
    "        if i<hk:\n",
    "            window=[]\n",
    "            j = 0\n",
    "            while j<(hk-i):\n",
    "                window.append('pad')\n",
    "                j=j+1\n",
    "            for ii in word_list[:i]:\n",
    "                window.append(ii)\n",
    "            for ii in word_list[i:(i+hk+1)]:\n",
    "                window.append(ii)\n",
    "            if len(window)<k:\n",
    "                jj = len(window)\n",
    "                while jj<k:\n",
    "                    window.append('pad')\n",
    "                    jj+=1\n",
    "            windows.append(window)\n",
    "        elif i>(len(word_list)-(hk+1)):\n",
    "            window=[]\n",
    "            for ii in word_list[(i-hk):i]:\n",
    "                window.append(ii)\n",
    "            for ii in word_list[i:]:\n",
    "                window.append(ii)\n",
    "            windows.append(window)\n",
    "            if len(window)<k:\n",
    "                jj = len(window)\n",
    "                while jj<k:\n",
    "                    window.append('pad')\n",
    "                    jj+=1\n",
    "        else:\n",
    "            window=[]\n",
    "            for ii in word_list[(i-hk):i]:\n",
    "                window.append(ii)\n",
    "            for ii in word_list[i:(i+hk+1)]:\n",
    "                window.append(ii)\n",
    "            windows.append(window)\n",
    "    return windows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k=3\n",
    "\n",
    "X_train_window=[get_window(v, k) for v in X_train]\n",
    "X_train_windows =[]\n",
    "for i in X_train_window:\n",
    "    for j in i:\n",
    "        X_train_windows.append(j)\n",
    "\n",
    "X_train_encode=[]\n",
    "for s in X_train_windows:\n",
    "    l=[]\n",
    "    for i in s:\n",
    "        if word_index.get(i):\n",
    "            l.append(word_index.get(i))\n",
    "        else:\n",
    "            l.append(int(pad_index))\n",
    "    X_train_encode.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350355\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train_encode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_window=[get_window(v, k) for v in X_test]\n",
    "X_test_windows =[]\n",
    "for i in X_test_window:\n",
    "    for j in i:\n",
    "        X_test_windows.append(j)\n",
    "\n",
    "X_test_encode=[]\n",
    "for s in X_test_windows:\n",
    "    l=[]\n",
    "    for i in s:\n",
    "        if word_index.get(i):\n",
    "            l.append(word_index.get(i))\n",
    "        else:\n",
    "            l.append(int(pad_index))\n",
    "    X_test_encode.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107228\n"
     ]
    }
   ],
   "source": [
    "print(len(X_test_encode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional, Flatten\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_train_pad = pad_sequences(Y_train_encode, maxlen=15)\n",
    "X_train_pad = pad_sequences(X_train_encode, maxlen=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_test_pad = pad_sequences(Y_test_encode, maxlen=15)\n",
    "X_test_pad = pad_sequences(X_test_encode, maxlen=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 315319 samples, validate on 35036 samples\n",
      "Epoch 1/6\n",
      "315319/315319 [==============================] - 26s - loss: 0.2678 - acc: 0.9298 - val_loss: 0.2359 - val_acc: 0.9344\n",
      "Epoch 2/6\n",
      "315319/315319 [==============================] - 26s - loss: 0.1643 - acc: 0.9533 - val_loss: 0.2131 - val_acc: 0.9381\n",
      "Epoch 3/6\n",
      "315319/315319 [==============================] - 29s - loss: 0.1537 - acc: 0.9555 - val_loss: 0.2093 - val_acc: 0.9395\n",
      "Epoch 4/6\n",
      "315319/315319 [==============================] - 28s - loss: 0.1481 - acc: 0.9568 - val_loss: 0.2176 - val_acc: 0.9399\n",
      "Epoch 5/6\n",
      "315319/315319 [==============================] - 28s - loss: 0.1450 - acc: 0.9575 - val_loss: 0.2115 - val_acc: 0.9396\n",
      "Epoch 6/6\n",
      "315319/315319 [==============================] - 27s - loss: 0.1426 - acc: 0.9582 - val_loss: 0.2076 - val_acc: 0.9403\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2245c898c88>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "nb_epoch = 6\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                            300,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=k,\n",
    "                            trainable=False))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(15, activation = 'softmax'))\n",
    "#model.add(Dense(15))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train_pad, Y_train_pad, epochs=nb_epoch, batch_size=batch_size,  validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 3, 300)            10524600  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 900)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 15)                13515     \n",
      "=================================================================\n",
      "Total params: 10,538,115\n",
      "Trainable params: 13,515\n",
      "Non-trainable params: 10,524,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107200/107228 [============================>.] - ETA: 0sTest score: 0.4929166453061807\n",
      "Test accuracy: 0.8902711978281349\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test_pad, Y_test_pad)\n",
    "\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 315319 samples, validate on 35036 samples\n",
      "Epoch 1/6\n",
      "315319/315319 [==============================] - 41s - loss: 8.4272 - acc: 0.0560 - val_loss: 10.2682 - val_acc: 0.0358\n",
      "Epoch 2/6\n",
      "315319/315319 [==============================] - 33s - loss: 7.8756 - acc: 0.0610 - val_loss: 5.0173 - val_acc: 0.0674\n",
      "Epoch 3/6\n",
      "315319/315319 [==============================] - 33s - loss: 6.5081 - acc: 0.0490 - val_loss: 5.1859 - val_acc: 0.0528\n",
      "Epoch 4/6\n",
      "315319/315319 [==============================] - 34s - loss: 9.0344 - acc: 0.0767 - val_loss: 7.9681 - val_acc: 0.0660\n",
      "Epoch 5/6\n",
      "315319/315319 [==============================] - 34s - loss: 8.8970 - acc: 0.0789 - val_loss: 11.3672 - val_acc: 0.0618\n",
      "Epoch 6/6\n",
      "315319/315319 [==============================] - 33s - loss: 8.8672 - acc: 0.0855 - val_loss: 11.0014 - val_acc: 0.0395\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2245ca4b240>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "nb_epoch = 6\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                            300,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=k,\n",
    "                            trainable=False))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(35, activation = 'softmax'))\n",
    "model.add(Dense(15))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train_pad, Y_train_pad, epochs=nb_epoch, batch_size=batch_size,  validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 3, 300)            10524600  \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 900)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 35)                31535     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 15)                540       \n",
      "=================================================================\n",
      "Total params: 10,556,675\n",
      "Trainable params: 32,075\n",
      "Non-trainable params: 10,524,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106976/107228 [============================>.] - ETA: 0sTest score: 11.145494400189769\n",
      "Test accuracy: 0.04050248069534077\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test_pad, Y_test_pad)\n",
    "\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 315319 samples, validate on 35036 samples\n",
      "Epoch 1/10\n",
      "315319/315319 [==============================] - 23s - loss: nan - acc: 0.0205 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 2/10\n",
      "315319/315319 [==============================] - 23s - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 3/10\n",
      "315319/315319 [==============================] - 24s - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 4/10\n",
      "315319/315319 [==============================] - 23s - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 5/10\n",
      "315319/315319 [==============================] - 23s - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 6/10\n",
      "315319/315319 [==============================] - 23s - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 7/10\n",
      "315319/315319 [==============================] - 24s - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 8/10\n",
      "315319/315319 [==============================] - 23s - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 9/10\n",
      "315319/315319 [==============================] - 23s - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 10/10\n",
      "315319/315319 [==============================] - 24s - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2245df39828>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "nb_epoch = 10\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                            300,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=k,\n",
    "                            trainable=False))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(15, activation = 'relu'))\n",
    "#model.add(Dense(15))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train_pad, Y_train_pad, epochs=nb_epoch, batch_size=batch_size,  validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 3, 300)            10524600  \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 900)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 15)                13515     \n",
      "=================================================================\n",
      "Total params: 10,538,115\n",
      "Trainable params: 13,515\n",
      "Non-trainable params: 10,524,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106368/107228 [============================>.] - ETA: 0s ETA:  - ETA: Test score: nan\n",
      "Test accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test_pad, Y_test_pad)\n",
    "\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
