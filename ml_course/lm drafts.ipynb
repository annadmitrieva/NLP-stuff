{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import LSTM, TimeDistributed, Bidirectional\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Embedding, Flatten, Dropout\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import numpy as np\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corp = open('/media/sf_communalflat/ihatelinguistics/history.conll', encoding='utf-8').read()\n",
    "\n",
    "from conllu import parse, parse_tree\n",
    "from conllu import print_tree\n",
    "\n",
    "tree = parse(corp)\n",
    "\n",
    "import re\n",
    "p = re.compile('[а-яА-ЯёЁ]')\n",
    "p1=re.compile('[0-9]')\n",
    "\n",
    "lemmas=[]\n",
    "for i in tree:\n",
    "    for j in i:\n",
    "        if p.search(j['lemma']) and not p1.search(j['lemma']):\n",
    "            lemmas.append(j['lemma'].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "\n",
    "def generate_seq(model, tokenizer, max_length, seed_text, n_words):\n",
    "    in_text = seed_text\n",
    "    for _ in range(n_words):\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        encoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n",
    "        yhat = model.predict_classes(encoded, verbose=0)\n",
    "        out_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                out_word = word\n",
    "                break\n",
    "        in_text += ' ' + out_word\n",
    "    return in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 8134\n",
      "Total Sequences: 64202\n",
      "Max Sequence Length: 3\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-c345a9207a2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0msequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;31m# define model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mllecture/lib/python3.5/site-packages/keras/utils/np_utils.py\u001b[0m in \u001b[0;36mto_categorical\u001b[0;34m(y, num_classes)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mnum_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mcategorical\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mcategorical\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcategorical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([' '.join(i for i in lemmas)])\n",
    "encoded = tokenizer.texts_to_sequences([' '.join(i for i in lemmas)])[0]\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: %d' % vocab_size)\n",
    "sequences = list()\n",
    "for i in range(2, len(encoded)):\n",
    "    sequence = encoded[i-2:i+1]\n",
    "    sequences.append(sequence)\n",
    "print('Total Sequences: %d' % len(sequences))\n",
    "max_length = max([len(seq) for seq in sequences])\n",
    "sequences = pad_sequences(sequences, maxlen=max_length, padding='pre')\n",
    "print('Max Sequence Length: %d' % max_length)\n",
    "sequences = array(sequences)\n",
    "X, y = sequences[:,:-1],sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 10, input_length=max_length-1))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=50, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model.save('lemmas_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model2=load_model('lemmas_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "взгляд православный верующий\n"
     ]
    }
   ],
   "source": [
    "#так генерируется наиболее вероятное следующеее слово\n",
    "print(generate_seq(model2, tokenizer, max_length-1, 'взгляд православный', 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#получаем матрицу вероятностей появления для всех известных модели слов после заданного н-грамма\n",
    "def get_all_probs(model, tokenizer, max_length, seed_text):\n",
    "    in_text = seed_text\n",
    "    encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "    encoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n",
    "    yhat = model.predict(encoded, verbose=0)\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01513699,  0.01998728,  0.04498252,  0.0994028 ,  0.36261377], dtype=float32)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(get_all_probs(model2, tokenizer, max_length-1, 'взгляд православный')[0])[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1772"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(get_all_probs(model2, tokenizer, max_length-1, 'взгляд православный')[0]==0.36261377)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "#генерируем строки на замену\n",
    "\n",
    "inv_dict = {v: k for k, v in tokenizer.word_index.items()}\n",
    "from pymystem3 import Mystem\n",
    "m = Mystem()\n",
    "\n",
    "small=1e-10\n",
    "#give it lemmatized strings maybe\n",
    "def get_trigram(trigram_string):\n",
    "    suggestions=[]\n",
    "    #for string in trigram_strings:\n",
    "    prob=0.0\n",
    "    bigrams=' '.join(m.lemmatize(i)[0] for i in trigram_string[:-1])\n",
    "    last_word=m.lemmatize(trigram_string[-1])[0]\n",
    "    prob_matrix=get_all_probs(model2, tokenizer, max_length-1, bigrams)[0]\n",
    "    most_probable=np.sort(prob_matrix)[-100:]\n",
    "    if last_word in tokenizer.word_index.keys():\n",
    "        prob=prob_matrix[tokenizer.word_index[last_word]]\n",
    "    else:\n",
    "        prob=small\n",
    "    if prob in most_probable:\n",
    "        #pass if trigram's probability is high\n",
    "        pass\n",
    "    else:\n",
    "        for i in most_probable[-5:]:\n",
    "            ind=np.where(prob_matrix==i)[0][0]\n",
    "            suggestions.append(bigrams+' '+inv_dict[ind])\n",
    "    if suggestions:\n",
    "        return 'возможно неверная коллокация: {}, варианты замен: {}'.format(trigram_string, suggestions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_trigram(['взгляд', 'православный', 'верующий'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1772"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index['верующий']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "#тестируем\n",
    "test=open('бэд.txt', encoding='utf-8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text=[]\n",
    "for i in test.split():\n",
    "    i=i.lower()\n",
    "    if p.search(i) and not p1.search(i):\n",
    "        clean_text.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams=list(ngrams(clean_text,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"возможно неверная коллокация: ['всероссийский', 'умер', 'уже'], варианты замен: ['всероссийский умирать быть', 'всероссийский умирать г', 'всероссийский умирать до', 'всероссийский умирать якобсон', 'всероссийский умирать в']\""
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_trigram(list(trigrams[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
